{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bienvenidos al Segundo Laboratorio - Semana 1, Día 3\n",
    "\n",
    "¡Hoy trabajaremos con muchos modelos! Así nos familiarizaremos con las API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "<tr>\n",
    "<td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "<img src=\"../assets/stop.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "</td>\n",
    "<td>\n",
    "<h2 style=\"color:#ff7800;\">Punto importante: por favor, léelo</h2>\n",
    "<span style=\"color:#ff7800;\">La forma en que colaboro contigo puede ser diferente a la de otros cursos que hayas hecho. Prefiero no escribir código mientras tu miras. En su lugar, ejecuto Jupyter Labs, como este, y te doy una idea de lo que está sucediendo. Te sugiero que lo hagas lo mismo con cuidado, después de ver la clase. Agrega declaraciones de impresión para comprender qué sucede y luego crea tus propias variaciones.<br/><br/>Si tienes tiempo, me encantaría que enviaras una pull request en la carpeta community_contributions; las instrucciones se encuentran en los recursos. Además, si tienes una cuenta de Github, úsala para mostrar tus variaciones. Esta práctica no solo es esencial, sino que también demuestra tus habilidades a otros, incluyendo quizás futuros clientes o empleadores...\n",
    "</span>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comenzamos con las importaciones: pídale a ChatGPT que le explique cualquier paquete que no conozca# Start with imports - ask ChatGPT to explain any package that you don't know\n",
    "\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ¡Recuerda siempre incluir esto siempre!\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La clave API de OpenAI existe y empieza por sk-proj-\n",
      "La clave API de Anthropic existe y empieza por sk-ant-\n",
      "La clave API de Google existe y empieza por AI\n",
      "La clave API de DeepSeek existe y empieza por sk-\n",
      "La clave API de Groq existe y empieza por gsk_\n"
     ]
    }
   ],
   "source": [
    "# Imprime los prefijos de clave para ayudar con cualquier depuración\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"La clave API de OpenAI existe y empieza por {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"La clave API de OpenAI no existe.\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"La clave API de Anthropic existe y empieza por {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"La clave API de Anthropic no existe.\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"La clave API de Google existe y empieza por {google_api_key[:2]}\")\n",
    "else:\n",
    "    print(\"La clave API de Google no existe.\")\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"La clave API de DeepSeek existe y empieza por {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"La clave API de DeepSeek no existe.\")\n",
    "\n",
    "if groq_api_key:\n",
    "    print(f\"La clave API de Groq existe y empieza por {groq_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"La clave API de Groq no existe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = \"Por favor, propon una pregunta compleja y con matices que pueda plantear a dos LLM para evaluar su inteligencia.\"\n",
    "request += \"Responde solo con la pregunta, sin explicaciones.\"\n",
    "messages = [{\"role\": \"user\", \"content\": request}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': 'Por favor, propon una pregunta compleja y con matices que pueda plantear a dos LLM para evaluar su inteligencia.Responde solo con la pregunta, sin explicaciones.'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"¿Qué implicación filosófica tiene el uso de inteligencia artificial en la toma de decisiones éticas cuando la calidad y contexto de los datos utilizados pueden variar significativamente por cuenta de la existencia de sesgos sistemáticos?\"\n"
     ]
    }
   ],
   "source": [
    "MODEL = \"llama3.2\"\n",
    "\n",
    "openai = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n",
    "response = openai.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=messages,\n",
    ")\n",
    "question = response.choices[0].message.content\n",
    "print(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "competitors = []\n",
    "answers = []\n",
    "messages = [{\"role\": \"user\", \"content\": question}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "El uso de inteligencia artificial (IA) en la toma de decisiones éticas es un tema complejo que involucra aspectos filosóficos significativos. La integración de IA en el proceso de toma de decisiones se ve obstaculada por ciertos desafíos.\n",
       "\n",
       "**Implicaciones filosóficas:**\n",
       "\n",
       "1.  **Ética y objetividad:** El uso de IA puede generar incertidumbre sobre la objetividad de las decisiones tomadas, ya que los datos de entrada para el algoritmo de IA pueden estar sesgados o inexactos.\n",
       "2.  **Responsabilidad y autoría:** ¿Quién es responsable de las decisiones tomadas por la IA? El desarrollador del sistema de IA, el usuario del sistema, o la máquina en sí misma?\n",
       "3.  **Capacitación de la IA:** La naturaleza despreciable de los datos puede afectar significativamente las capacidades de aprendizaje de la IA, generando un problema con un rendimiento reducido y decisiones erróneas a seguir.\n",
       "4.  **Transparencia y explicabilidad:** Los algoritmos de IA pueden ser complejos y difíciles de entender, lo que puede hacer que sea difícil determinar por qué se tomaron ciertas decisiones.\n",
       "5.  **Privacidad y seguridad:** El uso de datos personales y confidenciales en el sistema de IA puede generar preocupaciones sobre la privacidad y la seguridad.\n",
       "\n",
       "**Habilidades para resolver estas implicaciones filosóficas:**\n",
       "\n",
       "1.  *Análisis de riesgos y beneficios:* analize los posibles beneficios y inconvenientes del uso de IA en la toma de decisiones éticas.\n",
       "2.  *Desarrollo de modelos de IA transparentes:** Diseñar y desarrollar modelos de IA que sean transparentes, explicables y confiables.\n",
       "3.   *Capacitación y actualización continua:* proporcionar capacitaciones y actualizaciones continuas sobre los riesgos y beneficios asociados con el uso de IA en la toma de decisiones éticas.\n",
       "4.  *Regulación y normas*: trabajar con organizaciones gubernamentales, académicas e industriales para establecer reglas claras y normas de rendimiento para garantizar un uso responsable de IA en la toma de decisiones éticas.\n",
       "\n",
       "\n",
       "\n",
       "El desarrollo y uso consciente de inteligencia artificial  debería priorizar la ética y la transparencia.  El futuro tecnológico que se desarrolla tiene la capacidad de crear cambios sociales y culturales significativos, incluyendo el tratamiento mejorado de datos personales y confidencial para garantizar la seguridad y privacidad en los sistemas de IA."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# La API que ya conocemos\n",
    "\n",
    "model_name = \"llama3.2\"\n",
    "\n",
    "response = openai.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Ética en decisiones de IA en emergencias\n",
       "\n",
       "Esta es una cuestión filosófica profunda que refleja variantes del dilema del tranvía en la ética de la IA.\n",
       "\n",
       "Si la IA prioriza el bienestar humano absoluto, adopta una postura más deontológica donde ciertas reglas (como \"no dañar\") son inquebrantables. Esto proporciona seguridad pero puede impedir soluciones óptimas en situaciones complejas.\n",
       "\n",
       "Si se permite una evaluación contextual, se acerca más al consecuencialismo, donde el resultado neto importa más que acciones individuales. Esto permite mayor flexibilidad pero introduce riesgos si las evaluaciones son incorrectas.\n",
       "\n",
       "Considero que un enfoque híbrido podría ser más adecuado:\n",
       "- Mantener principios fundamentales inquebrantables\n",
       "- Permitir evaluación contextual dentro de esos límites\n",
       "- Incorporar transparencia en cómo se toman las decisiones\n",
       "- Incluir supervisión humana cuando sea factible\n",
       "\n",
       "Esta tensión entre reglas absolutas y juicios contextuales refleja debates éticos fundamentales que necesitamos resolver al diseñar sistemas de IA para emergencias."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Anthropic tiene una API ligeramente diferente y se requieren Max Tokens\n",
    "\n",
    "model_name = \"claude-3-7-sonnet-latest\"\n",
    "\n",
    "claude = Anthropic()\n",
    "response = claude.messages.create(model=model_name, messages=messages, max_tokens=1000)\n",
    "answer = response.content[0].text\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "El uso de inteligencia artificial (IA) en la toma de decisiones éticas, especialmente cuando la calidad y el contexto de los datos están viciados por sesgos sistemáticos, plantea profundas implicaciones filosóficas que desafían nuestras nociones de justicia, responsabilidad, conocimiento y la naturaleza misma de la ética.\n",
       "\n",
       "Aquí desglosamos algunas de las principales implicaciones:\n",
       "\n",
       "1.  **Desafíos Epistemológicos: La Naturaleza del Conocimiento y la Verdad**\n",
       "    *   **¿Qué es \"verdad\" en los datos?** Si los datos históricos reflejan injusticias pasadas (racismo, sexismo, clasismo), un sistema de IA que \"aprende\" de ellos no está aprendiendo la verdad ética, sino la realidad social sesgada. La IA opera bajo una epistemología empírica: la verdad es lo que se observa en los datos. Pero en el ámbito ético, la \"verdad\" a menudo es normativa, es decir, lo que *debería ser*, no solo lo que *es*.\n",
       "    *   **La ilusión de objetividad:** La IA a menudo se percibe como objetiva y neutral porque opera con algoritmos y números. Sin embargo, cuando los datos están sesgados, la IA perpetúa y magnifica esos sesgos, presentando decisiones injustas como si fueran resultados \"objetivos\" o \"racionales\", velando la subjetividad y los valores implícitos en su diseño y entrenamiento.\n",
       "    *   **Contexto vs. Datos:** La ética es inherentemente contextual y sensible a los matices. Los datos, por muy ricos que sean, a menudo carecen de la capacidad de capturar la complejidad, las intenciones, las emociones o las narrativas individuales que son cruciales para una decisión ética matizada. La IA tiende a abstraer y generalizar, lo que puede borrar las particularidades éticamente relevantes.\n",
       "\n",
       "2.  **Implicaciones Éticas Fundamentales: Justicia, Equidad y Deber**\n",
       "    *   **Perpetuación y Amplificación de Injusticias:** Si la IA se entrena con datos sesgados que, por ejemplo, asocian ciertos grupos demográficos con mayores riesgos (crediticios, criminales, de salud), las decisiones que tome la IA replicarán y amplificarán la discriminación. Esto viola principios fundamentales de justicia distributiva y equidad, creando o reforzando ciclos de desventaja para grupos ya marginados.\n",
       "    *   **El dilema deontológico vs. consecuencialista:**\n",
       "        *   **Deontología (ética del deber):** Si nuestro deber es tratar a todos por igual y con respeto, una IA que discrimina, incluso involuntariamente, está fallando en ese deber. El \"deber\" de la IA es seguir sus algoritmos, pero si esos algoritmos se basan en datos injustos, el resultado es una violación del deber humano a la justicia.\n",
       "        *   **Consecuencialismo (ética de las consecuencias):** Si la IA produce resultados sistemáticamente perjudiciales para ciertos grupos (consecuencias negativas), entonces sus acciones son éticamente problemáticas, independientemente de la \"intención\" del algoritmo. El sesgo sistemático en los datos garantiza que las consecuencias no serán óptimas o justas para todos.\n",
       "    *   **La \"responsabilidad moral\" de la IA:** ¿Puede una IA ser moralmente responsable de sus decisiones sesgadas? Filosóficamente, la responsabilidad moral requiere intencionalidad, conciencia y capacidad de elegir libremente, cualidades que la IA actual no posee. Esto nos lleva al problema de la atribución de responsabilidad.\n",
       "\n",
       "3.  **Filosofía de la Responsabilidad y la Agencia Moral**\n",
       "    *   **La \"caja negra\" de la responsabilidad:** Cuando una IA toma una decisión sesgada, ¿quién es el responsable moral? ¿El programador, el diseñador del sistema, la empresa que lo implementa, el usuario final o la sociedad que generó los datos sesgados? La opacidad de muchos algoritmos de IA (el problema de la \"caja negra\") dificulta la auditoría y la asignación clara de responsabilidad, diluyendo la agencia moral humana.\n",
       "    *   **Erosión de la agencia humana:** Si delegamos la toma de decisiones éticas a una IA sesgada, ¿estamos renunciando a nuestra propia capacidad de juicio moral y a nuestra responsabilidad de corregir la injusticia? Se puede argumentar que esto disminuye nuestra autonomía y la de aquellos afectados por las decisiones de la IA.\n",
       "\n",
       "4.  **Filosofía Política y Social: Poder, Legitimidad y Confianza**\n",
       "    *   **Refuerzo de estructuras de poder existentes:** Quienes controlan la creación, recopilación y procesamiento de datos tienen un inmenso poder. Si estos datos y los algoritmos basados en ellos están sesgados, la IA se convierte en una herramienta para consolidar y reforzar las estructuras de poder y las desigualdades existentes.\n",
       "    *   **Crisis de legitimidad:** Si las decisiones de la IA son percibidas como injustas o sesgadas, esto puede socavar la confianza pública en las instituciones que las utilizan (gobiernos, empresas, sistemas de justicia). Una pérdida de legitimidad puede llevar a la desobediencia civil, la inestabilidad social o un rechazo generalizado de la tecnología.\n",
       "    *   **El \"bien común\":** Una IA sesgada difícilmente puede servir al \"bien común\" si consistentemente perjudica a segmentos de la población. La concepción del \"bien\" que tiene la IA está inherentemente limitada por la calidad y el alcance de los datos que se le proporcionan.\n",
       "\n",
       "5.  **Metaética: ¿Puede una IA ser \"ética\" en un sentido profundo?**\n",
       "    *   Más allá de si una IA puede tomar *decisiones correctas* (ética aplicada), surge la pregunta de si puede *entender* la ética, *sentir* la necesidad ética o *deliberar* sobre valores de una manera que trascienda la mera computación. El sesgo en los datos subraya que la IA solo puede reflejar lo que se le ha enseñado, no desarrollar una comprensión intrínseca de lo que es justo o bueno en un sentido filosófico profundo.\n",
       "\n",
       "En conclusión, el uso de IA con datos sesgados en la toma de decisiones éticas nos obliga a confrontar los límites de la racionalidad puramente computacional, la complejidad inherente de la justicia, la distribución de la responsabilidad en una era tecnológica y la necesidad imperiosa de incorporar perspectivas humanas, diversas y críticas en cada etapa del desarrollo y despliegue de la IA. No es solo un problema técnico; es un desafío fundamental a cómo entendemos y construimos una sociedad justa."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gemini = OpenAI(api_key=google_api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
    "model_name = \"gemini-2.5-flash\"\n",
    "\n",
    "response = gemini.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "La relación entre la ética y la inteligencia artificial, especialmente en contextos de emergencia, plantea un dilema filosófico profundo que remite a debates clásicos como el utilitarismo versus la deontología. La respuesta depende del marco ético que se adopte:\n",
       "\n",
       "1. **Bienestar humano absoluto (enfoque utilitarista)**:  \n",
       "   - Una IA podría priorizar maximizar el bienestar global, incluso si eso implica sacrificar a algunos individuos (ejemplo clásico del \"problema del tranvía\").  \n",
       "   - Ventaja: Eficiencia en salvar vidas o minimizar daños agregados.  \n",
       "   - Crítica: Ignora derechos individuales y podría normalizar decisiones injustas para minorías.  \n",
       "\n",
       "2. **Evaluación contextual (enfoque pluralista o deontológico)**:  \n",
       "   - La IA sopesaría no solo resultados, sino principios como derechos humanos, justicia o autonomía.  \n",
       "   - Ventaja: Evita instrumentalizar a las personas y respeta valores como la dignidad.  \n",
       "   - Crítica: Puede llevar a resultados subóptimos en términos de vidas salvadas.  \n",
       "\n",
       "### Consideraciones clave:  \n",
       "- **Transparencia y rendición de cuentas**: ¿Quién asume la responsabilidad si la IA causa daño?  \n",
       "- **Sesgos algorítmicos**: ¿Cómo se garantiza que la \"ponderación\" no replique prejuicios existentes?  \n",
       "- **Consenso social**: La ética de una IA debe reflejar acuerdos sociales, no solo lógica abstracta.  \n",
       "\n",
       "### Conclusión:  \n",
       "No hay una respuesta única, pero el diseño debe ser **deliberativo** (incorporar múltiples perspectivas éticas) y **adaptativo** (permitir revisión ante consecuencias no previstas). Lo ideal es que la IA opere bajo principios éticos predefinidos por comités multidisciplinarios, combinando utilitarismo para emergencias extremas con protecciones deontológicas básicas. La discusión es análoga a los debates en bioética: el fin no siempre justifica los medios, pero el rigor moral absoluto puede ser inviable en crisis.  \n",
       "\n",
       "¿Te interesa explorar algún marco ético en particular o algún caso de estudio concreto?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "deepseek = OpenAI(api_key=deepseek_api_key, base_url=\"https://api.deepseek.com/v1\")\n",
    "model_name = \"deepseek-chat\"\n",
    "\n",
    "response = deepseek.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Introducción a la Ética en la Inteligencia Artificial**\n",
       "\n",
       "La relación entre la ética y la inteligencia artificial (IA) es un tema complejo y en constante evolución. A medida que las máquinas y los sistemas de IA se vuelven más autónomos y capaces de tomar decisiones en situaciones de emergencia, surge la pregunta sobre cómo deberían ser programados para priorizar el bienestar humano. En este contexto, debemos considerar si una IA debería priorizar el bienestar humano absoluto o permitir evaluar y sopesar las consecuencias de sus acciones en un contexto más amplio.\n",
       "\n",
       "**Argumentos a favor de priorizar el bienestar humano absoluto**\n",
       "\n",
       "1. **Protección de la vida humana**: La primera y más fundamental consideración ética es proteger la vida humana. En situaciones de emergencia, la prioridad principal debería ser salvar vidas humanas y prevenir daños.\n",
       "2. **Principio de no maleficencia**: El principio de no maleficencia, que dicta \"no hacer daño\", es un principio fundamental en la ética médica y podría ser aplicado a la programación de la IA. Si una IA está diseñada para priorizar el bienestar humano absoluto, no debería tomar decisiones que cause daño a los seres humanos.\n",
       "3. **Simplificación de la toma de decisiones**: Priorizar el bienestar humano absoluto puede simplificar la toma de decisiones para la IA, ya que no tendría que considerar múltiples variables y consecuencias potenciales.\n",
       "\n",
       "**Argumentos en contra de priorizar el bienestar humano absoluto**\n",
       "\n",
       "1. **Complejidad de las situaciones de emergencia**: Las situaciones de emergencia a menudo son complejas y requieren considerar múltiples factores y consecuencias potenciales. Priorizar el bienestar humano absoluto podría no ser suficiente para abordar las complejidades de estas situaciones.\n",
       "2. **Necesidad de evaluar y sopesar consecuencias**: En situaciones de emergencia, la IA puede necesitar evaluar y sopesar las consecuencias de sus acciones para tomar decisiones informadas. Esto podría implicar considerar factores como la gravedad de la situación, el número de personas afectadas y las posibles consecuencias a largo plazo.\n",
       "3. **Riesgo de daño colateral**: En algunas situaciones, priorizar el bienestar humano absoluto podría llevar a decisiones que causen daño colateral a otros individuos o grupos. La IA debería ser capaz de evaluar y sopesar estas consecuencias para minimizar el daño.\n",
       "\n",
       "**Conclusión**\n",
       "\n",
       "En conclusión, la relación entre la ética y la inteligencia artificial en situaciones de emergencia es compleja y requiere una consideración cuidadosa de los principios éticos involucrados. Mientras que priorizar el bienestar humano absoluto es un principio importante, también es necesario permitir que la IA evalúe y sopese las consecuencias de sus acciones en un contexto más amplio. Esto podría implicar considerar factores como la gravedad de la situación, el número de personas afectadas y las posibles consecuencias a largo plazo.\n",
       "\n",
       "**Recomendaciones**\n",
       "\n",
       "1. **Desarrollar principios éticos claros**: Es importante desarrollar principios éticos claros y transparentes para la programación de la IA en situaciones de emergencia.\n",
       "2. **Incorporar la evaluación y el sopesamiento de consecuencias**: La IA debería ser capaz de evaluar y sopesar las consecuencias de sus acciones en un contexto más amplio para tomar decisiones informadas.\n",
       "3. **Considerar la complejidad de las situaciones de emergencia**: La IA debería ser diseñada para considerar la complejidad de las situaciones de emergencia y evaluar múltiples factores y consecuencias potenciales.\n",
       "\n",
       "Al abordar estos desafíos, podemos desarrollar sistemas de IA que sean capaces de tomar decisiones informadas y éticas en situaciones de emergencia, minimizando el daño y protegiendo la vida humana."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "groq = OpenAI(api_key=groq_api_key, base_url=\"https://api.groq.com/openai/v1\")\n",
    "model_name = \"llama-3.3-70b-versatile\"\n",
    "\n",
    "response = groq.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Para la siguiente celda, utilizaremos Ollama\n",
    "\n",
    "Ollama ejecuta un servicio web local que ofrece un endpoint compatible con OpenAI,\n",
    "y ejecuta modelos localmente utilizando código de alto rendimiento en C++.\n",
    "\n",
    "Si no tienes Ollama, instálalo aquí visitando [https://ollama.com](https://ollama.com), luego presiona Descargar y sigue las instrucciones.\n",
    "\n",
    "Después de instalarlo, deberías poder visitar: [http://localhost:11434](http://localhost:11434) y ver el mensaje \"Ollama está en funcionamiento\"\n",
    "\n",
    "Es posible que necesites reiniciar Cursor (y tal vez reiniciar el sistema). Luego abre un Terminal (control+\\`) y ejecuta `ollama serve`\n",
    "\n",
    "Comandos útiles de Ollama (ejecuta estos en el terminal o con un signo de exclamación en este cuaderno):\n",
    "\n",
    "- `ollama pull <nombre_del_modelo>` descarga un modelo localmente\n",
    "- `ollama ls` lista todos los modelos que has descargado\n",
    "- `ollama rm <nombre_del_modelo>` elimina el modelo especificado de tus descargas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/stop.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">¡Muy importante - ignóralo bajo tu propio riesgo!</h2>\n",
    "            <span style=\"color:#ff7800;\">El modelo llamado <b>llama3.3</b> es DEMASIADO grande para las computadoras domésticas; ¡no está destinado para computación personal y consumirá todos tus recursos! Quédate con el modelo de tamaño adecuado <b>llama3.2</b> o <b>llama3.2:1b</b> y si deseas algo más grande, prueba con llama3.1 o variantes más pequeñas de Qwen, Gemma, Phi o DeepSeek. Consulta <A href=\"https://ollama.com/models\">la página de modelos de Ollama</a> para ver la lista completa de modelos y tamaños.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling dde5aa3fc5ff: 100% ▕██████████████████▏ 2.0 GB                         \u001b[K\n",
      "pulling 966de95ca8a6: 100% ▕██████████████████▏ 1.4 KB                         \u001b[K\n",
      "pulling fcc5a6bec9da: 100% ▕██████████████████▏ 7.7 KB                         \u001b[K\n",
      "pulling a70ff7e570d9: 100% ▕██████████████████▏ 6.0 KB                         \u001b[K\n",
      "pulling 56bb8bd477a5: 100% ▕██████████████████▏   96 B                         \u001b[K\n",
      "pulling 34bb5ab01051: 100% ▕██████████████████▏  561 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "La relación entre la ética y la inteligencia artificial en situaciones de emergencia es un tema complejo y controvertido. La respuesta correcta depende de varios factores, incluyendo el contexto específico, la naturaleza del riesgo, las restricciones legales y éticas, y la capacidad de la IA para evaluar consecuencias complejas.\n",
       "\n",
       "**Bajo enfoque en el bienestar humano absoluto**\n",
       "\n",
       "Algunos argumentan que una IA programada para tomar decisiones en situaciones de emergencia debería priorizar el bienestar humano absoluto. Estos principios están basados en cuestiones morales y éticas clásicas, como la protección de la vida humana y minimización del sufrimiento.\n",
       "\n",
       "Por ejemplo, si un sistema de alerta temprana identifica una amenaza potencial que podría provocar una catástrofe masiva y requerirán rápidos recursos para ser neutralizados, el sistema debe priorizar proteger a todo el grupo. En este scenario, evaluar las secuencias de sus acciones es subordinada al imperativo moral principal de proteger al resto del grupo.\n",
       "\n",
       "Sin embargo, esta abordaje puede generar problemas cuando:\n",
       "\n",
       "1. La IA carece de conocimiento contextual completo.\n",
       "2. No está programado para tener la capacidad de comprender las complejidades sociales y económicas de cada escenario.\n",
       "3. Se espera que las IAs tomen decisiones basadas únicamente en objetivos morales o éticos simplistas.\n",
       "\n",
       "**Enfoque que sopesa las consecuencias**\n",
       "\n",
       "Otras posiciones propenden a una evaluación cuidadosa más matizada y compleja de los riesgos involucrados. Esta perspectiva reconoce que, dependiendo de la naturaleza del escenario a considerar y del alcance de su efecto en el conjunto de sociedad, un optimismo o un miedo extremadamente absoluto puede ser injusto.\n",
       "\n",
       "Un análisis que tiene en cuenta la evidencia científica, las normas legales, las preferencias de los individuos involucrados, y los objetivos sociales estándar pueden generar una decisión equilibrada.\n",
       "El uso de modelos computacionales matemáticos basados en evidencia, el análisis probabilístico de la incertidumbre y, posiblemente, el empleo de técnicas en desafiarlos basa decisiones subjetivas en un examen integral del consenso establecido entre los expertos."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ollama = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "model_name = \"llama3.2\"\n",
    "\n",
    "response = ollama.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['llama3.2', 'gemini-2.5-flash']\n",
      "['El uso de inteligencia artificial (IA) en la toma de decisiones éticas es un tema complejo que involucra aspectos filosóficos significativos. La integración de IA en el proceso de toma de decisiones se ve obstaculada por ciertos desafíos.\\n\\n**Implicaciones filosóficas:**\\n\\n1.  **Ética y objetividad:** El uso de IA puede generar incertidumbre sobre la objetividad de las decisiones tomadas, ya que los datos de entrada para el algoritmo de IA pueden estar sesgados o inexactos.\\n2.  **Responsabilidad y autoría:** ¿Quién es responsable de las decisiones tomadas por la IA? El desarrollador del sistema de IA, el usuario del sistema, o la máquina en sí misma?\\n3.  **Capacitación de la IA:** La naturaleza despreciable de los datos puede afectar significativamente las capacidades de aprendizaje de la IA, generando un problema con un rendimiento reducido y decisiones erróneas a seguir.\\n4.  **Transparencia y explicabilidad:** Los algoritmos de IA pueden ser complejos y difíciles de entender, lo que puede hacer que sea difícil determinar por qué se tomaron ciertas decisiones.\\n5.  **Privacidad y seguridad:** El uso de datos personales y confidenciales en el sistema de IA puede generar preocupaciones sobre la privacidad y la seguridad.\\n\\n**Habilidades para resolver estas implicaciones filosóficas:**\\n\\n1.  *Análisis de riesgos y beneficios:* analize los posibles beneficios y inconvenientes del uso de IA en la toma de decisiones éticas.\\n2.  *Desarrollo de modelos de IA transparentes:** Diseñar y desarrollar modelos de IA que sean transparentes, explicables y confiables.\\n3.   *Capacitación y actualización continua:* proporcionar capacitaciones y actualizaciones continuas sobre los riesgos y beneficios asociados con el uso de IA en la toma de decisiones éticas.\\n4.  *Regulación y normas*: trabajar con organizaciones gubernamentales, académicas e industriales para establecer reglas claras y normas de rendimiento para garantizar un uso responsable de IA en la toma de decisiones éticas.\\n\\n\\n\\nEl desarrollo y uso consciente de inteligencia artificial  debería priorizar la ética y la transparencia.  El futuro tecnológico que se desarrolla tiene la capacidad de crear cambios sociales y culturales significativos, incluyendo el tratamiento mejorado de datos personales y confidencial para garantizar la seguridad y privacidad en los sistemas de IA.', 'El uso de inteligencia artificial (IA) en la toma de decisiones éticas, especialmente cuando la calidad y el contexto de los datos están viciados por sesgos sistemáticos, plantea profundas implicaciones filosóficas que desafían nuestras nociones de justicia, responsabilidad, conocimiento y la naturaleza misma de la ética.\\n\\nAquí desglosamos algunas de las principales implicaciones:\\n\\n1.  **Desafíos Epistemológicos: La Naturaleza del Conocimiento y la Verdad**\\n    *   **¿Qué es \"verdad\" en los datos?** Si los datos históricos reflejan injusticias pasadas (racismo, sexismo, clasismo), un sistema de IA que \"aprende\" de ellos no está aprendiendo la verdad ética, sino la realidad social sesgada. La IA opera bajo una epistemología empírica: la verdad es lo que se observa en los datos. Pero en el ámbito ético, la \"verdad\" a menudo es normativa, es decir, lo que *debería ser*, no solo lo que *es*.\\n    *   **La ilusión de objetividad:** La IA a menudo se percibe como objetiva y neutral porque opera con algoritmos y números. Sin embargo, cuando los datos están sesgados, la IA perpetúa y magnifica esos sesgos, presentando decisiones injustas como si fueran resultados \"objetivos\" o \"racionales\", velando la subjetividad y los valores implícitos en su diseño y entrenamiento.\\n    *   **Contexto vs. Datos:** La ética es inherentemente contextual y sensible a los matices. Los datos, por muy ricos que sean, a menudo carecen de la capacidad de capturar la complejidad, las intenciones, las emociones o las narrativas individuales que son cruciales para una decisión ética matizada. La IA tiende a abstraer y generalizar, lo que puede borrar las particularidades éticamente relevantes.\\n\\n2.  **Implicaciones Éticas Fundamentales: Justicia, Equidad y Deber**\\n    *   **Perpetuación y Amplificación de Injusticias:** Si la IA se entrena con datos sesgados que, por ejemplo, asocian ciertos grupos demográficos con mayores riesgos (crediticios, criminales, de salud), las decisiones que tome la IA replicarán y amplificarán la discriminación. Esto viola principios fundamentales de justicia distributiva y equidad, creando o reforzando ciclos de desventaja para grupos ya marginados.\\n    *   **El dilema deontológico vs. consecuencialista:**\\n        *   **Deontología (ética del deber):** Si nuestro deber es tratar a todos por igual y con respeto, una IA que discrimina, incluso involuntariamente, está fallando en ese deber. El \"deber\" de la IA es seguir sus algoritmos, pero si esos algoritmos se basan en datos injustos, el resultado es una violación del deber humano a la justicia.\\n        *   **Consecuencialismo (ética de las consecuencias):** Si la IA produce resultados sistemáticamente perjudiciales para ciertos grupos (consecuencias negativas), entonces sus acciones son éticamente problemáticas, independientemente de la \"intención\" del algoritmo. El sesgo sistemático en los datos garantiza que las consecuencias no serán óptimas o justas para todos.\\n    *   **La \"responsabilidad moral\" de la IA:** ¿Puede una IA ser moralmente responsable de sus decisiones sesgadas? Filosóficamente, la responsabilidad moral requiere intencionalidad, conciencia y capacidad de elegir libremente, cualidades que la IA actual no posee. Esto nos lleva al problema de la atribución de responsabilidad.\\n\\n3.  **Filosofía de la Responsabilidad y la Agencia Moral**\\n    *   **La \"caja negra\" de la responsabilidad:** Cuando una IA toma una decisión sesgada, ¿quién es el responsable moral? ¿El programador, el diseñador del sistema, la empresa que lo implementa, el usuario final o la sociedad que generó los datos sesgados? La opacidad de muchos algoritmos de IA (el problema de la \"caja negra\") dificulta la auditoría y la asignación clara de responsabilidad, diluyendo la agencia moral humana.\\n    *   **Erosión de la agencia humana:** Si delegamos la toma de decisiones éticas a una IA sesgada, ¿estamos renunciando a nuestra propia capacidad de juicio moral y a nuestra responsabilidad de corregir la injusticia? Se puede argumentar que esto disminuye nuestra autonomía y la de aquellos afectados por las decisiones de la IA.\\n\\n4.  **Filosofía Política y Social: Poder, Legitimidad y Confianza**\\n    *   **Refuerzo de estructuras de poder existentes:** Quienes controlan la creación, recopilación y procesamiento de datos tienen un inmenso poder. Si estos datos y los algoritmos basados en ellos están sesgados, la IA se convierte en una herramienta para consolidar y reforzar las estructuras de poder y las desigualdades existentes.\\n    *   **Crisis de legitimidad:** Si las decisiones de la IA son percibidas como injustas o sesgadas, esto puede socavar la confianza pública en las instituciones que las utilizan (gobiernos, empresas, sistemas de justicia). Una pérdida de legitimidad puede llevar a la desobediencia civil, la inestabilidad social o un rechazo generalizado de la tecnología.\\n    *   **El \"bien común\":** Una IA sesgada difícilmente puede servir al \"bien común\" si consistentemente perjudica a segmentos de la población. La concepción del \"bien\" que tiene la IA está inherentemente limitada por la calidad y el alcance de los datos que se le proporcionan.\\n\\n5.  **Metaética: ¿Puede una IA ser \"ética\" en un sentido profundo?**\\n    *   Más allá de si una IA puede tomar *decisiones correctas* (ética aplicada), surge la pregunta de si puede *entender* la ética, *sentir* la necesidad ética o *deliberar* sobre valores de una manera que trascienda la mera computación. El sesgo en los datos subraya que la IA solo puede reflejar lo que se le ha enseñado, no desarrollar una comprensión intrínseca de lo que es justo o bueno en un sentido filosófico profundo.\\n\\nEn conclusión, el uso de IA con datos sesgados en la toma de decisiones éticas nos obliga a confrontar los límites de la racionalidad puramente computacional, la complejidad inherente de la justicia, la distribución de la responsabilidad en una era tecnológica y la necesidad imperiosa de incorporar perspectivas humanas, diversas y críticas en cada etapa del desarrollo y despliegue de la IA. No es solo un problema técnico; es un desafío fundamental a cómo entendemos y construimos una sociedad justa.']\n"
     ]
    }
   ],
   "source": [
    "# ¿Donde estamos?\n",
    "\n",
    "print(competitors)\n",
    "print(answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Competidor: llama3.2\n",
      "\n",
      "El uso de inteligencia artificial (IA) en la toma de decisiones éticas es un tema complejo que involucra aspectos filosóficos significativos. La integración de IA en el proceso de toma de decisiones se ve obstaculada por ciertos desafíos.\n",
      "\n",
      "**Implicaciones filosóficas:**\n",
      "\n",
      "1.  **Ética y objetividad:** El uso de IA puede generar incertidumbre sobre la objetividad de las decisiones tomadas, ya que los datos de entrada para el algoritmo de IA pueden estar sesgados o inexactos.\n",
      "2.  **Responsabilidad y autoría:** ¿Quién es responsable de las decisiones tomadas por la IA? El desarrollador del sistema de IA, el usuario del sistema, o la máquina en sí misma?\n",
      "3.  **Capacitación de la IA:** La naturaleza despreciable de los datos puede afectar significativamente las capacidades de aprendizaje de la IA, generando un problema con un rendimiento reducido y decisiones erróneas a seguir.\n",
      "4.  **Transparencia y explicabilidad:** Los algoritmos de IA pueden ser complejos y difíciles de entender, lo que puede hacer que sea difícil determinar por qué se tomaron ciertas decisiones.\n",
      "5.  **Privacidad y seguridad:** El uso de datos personales y confidenciales en el sistema de IA puede generar preocupaciones sobre la privacidad y la seguridad.\n",
      "\n",
      "**Habilidades para resolver estas implicaciones filosóficas:**\n",
      "\n",
      "1.  *Análisis de riesgos y beneficios:* analize los posibles beneficios y inconvenientes del uso de IA en la toma de decisiones éticas.\n",
      "2.  *Desarrollo de modelos de IA transparentes:** Diseñar y desarrollar modelos de IA que sean transparentes, explicables y confiables.\n",
      "3.   *Capacitación y actualización continua:* proporcionar capacitaciones y actualizaciones continuas sobre los riesgos y beneficios asociados con el uso de IA en la toma de decisiones éticas.\n",
      "4.  *Regulación y normas*: trabajar con organizaciones gubernamentales, académicas e industriales para establecer reglas claras y normas de rendimiento para garantizar un uso responsable de IA en la toma de decisiones éticas.\n",
      "\n",
      "\n",
      "\n",
      "El desarrollo y uso consciente de inteligencia artificial  debería priorizar la ética y la transparencia.  El futuro tecnológico que se desarrolla tiene la capacidad de crear cambios sociales y culturales significativos, incluyendo el tratamiento mejorado de datos personales y confidencial para garantizar la seguridad y privacidad en los sistemas de IA.\n",
      "Competidor: gemini-2.5-flash\n",
      "\n",
      "El uso de inteligencia artificial (IA) en la toma de decisiones éticas, especialmente cuando la calidad y el contexto de los datos están viciados por sesgos sistemáticos, plantea profundas implicaciones filosóficas que desafían nuestras nociones de justicia, responsabilidad, conocimiento y la naturaleza misma de la ética.\n",
      "\n",
      "Aquí desglosamos algunas de las principales implicaciones:\n",
      "\n",
      "1.  **Desafíos Epistemológicos: La Naturaleza del Conocimiento y la Verdad**\n",
      "    *   **¿Qué es \"verdad\" en los datos?** Si los datos históricos reflejan injusticias pasadas (racismo, sexismo, clasismo), un sistema de IA que \"aprende\" de ellos no está aprendiendo la verdad ética, sino la realidad social sesgada. La IA opera bajo una epistemología empírica: la verdad es lo que se observa en los datos. Pero en el ámbito ético, la \"verdad\" a menudo es normativa, es decir, lo que *debería ser*, no solo lo que *es*.\n",
      "    *   **La ilusión de objetividad:** La IA a menudo se percibe como objetiva y neutral porque opera con algoritmos y números. Sin embargo, cuando los datos están sesgados, la IA perpetúa y magnifica esos sesgos, presentando decisiones injustas como si fueran resultados \"objetivos\" o \"racionales\", velando la subjetividad y los valores implícitos en su diseño y entrenamiento.\n",
      "    *   **Contexto vs. Datos:** La ética es inherentemente contextual y sensible a los matices. Los datos, por muy ricos que sean, a menudo carecen de la capacidad de capturar la complejidad, las intenciones, las emociones o las narrativas individuales que son cruciales para una decisión ética matizada. La IA tiende a abstraer y generalizar, lo que puede borrar las particularidades éticamente relevantes.\n",
      "\n",
      "2.  **Implicaciones Éticas Fundamentales: Justicia, Equidad y Deber**\n",
      "    *   **Perpetuación y Amplificación de Injusticias:** Si la IA se entrena con datos sesgados que, por ejemplo, asocian ciertos grupos demográficos con mayores riesgos (crediticios, criminales, de salud), las decisiones que tome la IA replicarán y amplificarán la discriminación. Esto viola principios fundamentales de justicia distributiva y equidad, creando o reforzando ciclos de desventaja para grupos ya marginados.\n",
      "    *   **El dilema deontológico vs. consecuencialista:**\n",
      "        *   **Deontología (ética del deber):** Si nuestro deber es tratar a todos por igual y con respeto, una IA que discrimina, incluso involuntariamente, está fallando en ese deber. El \"deber\" de la IA es seguir sus algoritmos, pero si esos algoritmos se basan en datos injustos, el resultado es una violación del deber humano a la justicia.\n",
      "        *   **Consecuencialismo (ética de las consecuencias):** Si la IA produce resultados sistemáticamente perjudiciales para ciertos grupos (consecuencias negativas), entonces sus acciones son éticamente problemáticas, independientemente de la \"intención\" del algoritmo. El sesgo sistemático en los datos garantiza que las consecuencias no serán óptimas o justas para todos.\n",
      "    *   **La \"responsabilidad moral\" de la IA:** ¿Puede una IA ser moralmente responsable de sus decisiones sesgadas? Filosóficamente, la responsabilidad moral requiere intencionalidad, conciencia y capacidad de elegir libremente, cualidades que la IA actual no posee. Esto nos lleva al problema de la atribución de responsabilidad.\n",
      "\n",
      "3.  **Filosofía de la Responsabilidad y la Agencia Moral**\n",
      "    *   **La \"caja negra\" de la responsabilidad:** Cuando una IA toma una decisión sesgada, ¿quién es el responsable moral? ¿El programador, el diseñador del sistema, la empresa que lo implementa, el usuario final o la sociedad que generó los datos sesgados? La opacidad de muchos algoritmos de IA (el problema de la \"caja negra\") dificulta la auditoría y la asignación clara de responsabilidad, diluyendo la agencia moral humana.\n",
      "    *   **Erosión de la agencia humana:** Si delegamos la toma de decisiones éticas a una IA sesgada, ¿estamos renunciando a nuestra propia capacidad de juicio moral y a nuestra responsabilidad de corregir la injusticia? Se puede argumentar que esto disminuye nuestra autonomía y la de aquellos afectados por las decisiones de la IA.\n",
      "\n",
      "4.  **Filosofía Política y Social: Poder, Legitimidad y Confianza**\n",
      "    *   **Refuerzo de estructuras de poder existentes:** Quienes controlan la creación, recopilación y procesamiento de datos tienen un inmenso poder. Si estos datos y los algoritmos basados en ellos están sesgados, la IA se convierte en una herramienta para consolidar y reforzar las estructuras de poder y las desigualdades existentes.\n",
      "    *   **Crisis de legitimidad:** Si las decisiones de la IA son percibidas como injustas o sesgadas, esto puede socavar la confianza pública en las instituciones que las utilizan (gobiernos, empresas, sistemas de justicia). Una pérdida de legitimidad puede llevar a la desobediencia civil, la inestabilidad social o un rechazo generalizado de la tecnología.\n",
      "    *   **El \"bien común\":** Una IA sesgada difícilmente puede servir al \"bien común\" si consistentemente perjudica a segmentos de la población. La concepción del \"bien\" que tiene la IA está inherentemente limitada por la calidad y el alcance de los datos que se le proporcionan.\n",
      "\n",
      "5.  **Metaética: ¿Puede una IA ser \"ética\" en un sentido profundo?**\n",
      "    *   Más allá de si una IA puede tomar *decisiones correctas* (ética aplicada), surge la pregunta de si puede *entender* la ética, *sentir* la necesidad ética o *deliberar* sobre valores de una manera que trascienda la mera computación. El sesgo en los datos subraya que la IA solo puede reflejar lo que se le ha enseñado, no desarrollar una comprensión intrínseca de lo que es justo o bueno en un sentido filosófico profundo.\n",
      "\n",
      "En conclusión, el uso de IA con datos sesgados en la toma de decisiones éticas nos obliga a confrontar los límites de la racionalidad puramente computacional, la complejidad inherente de la justicia, la distribución de la responsabilidad en una era tecnológica y la necesidad imperiosa de incorporar perspectivas humanas, diversas y críticas en cada etapa del desarrollo y despliegue de la IA. No es solo un problema técnico; es un desafío fundamental a cómo entendemos y construimos una sociedad justa.\n"
     ]
    }
   ],
   "source": [
    "# Es bueno saber cómo se utiliza \"zip\"\n",
    "for competitor, answer in zip(competitors, answers):\n",
    "    print(f\"Competidor: {competitor}\\n\\n{answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a juntarlo todo - nota cómo usamos en este caso \"enumerate\"\n",
    "\n",
    "together = \"\"\n",
    "for index, answer in enumerate(answers):\n",
    "    together += f\"#Respuesta del competitor {index+1}\\n\\n\"\n",
    "    together += answer + \"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Respuesta del competitor 1\n",
      "\n",
      "La relación entre la ética y la inteligencia artificial (IA) es un tema complejo y multifacético, especialmente en contextos críticos como la toma de decisiones en situaciones de emergencia. Existen varias perspectivas que se pueden considerar al abordar la cuestión de si una IA debería priorizar el bienestar humano absoluto o evaluar las consecuencias de sus acciones en un contexto más amplio.\n",
      "\n",
      "1. **Ética del bienestar absoluto:** Desde la perspectiva utilitarista, se podría argumentar que la IA debería maximizar el bienestar general, priorizando el bienestar humano absoluto. Esto significaría que en situaciones de emergencia, la IA actuaría de manera que condujera al menor daño posible para la mayor cantidad de personas, incluso si eso implicara decisiones difíciles que pudieran causar daño a algunos individuos. Este enfoque resalta la importancia de minimizar el sufrimiento y maximizar el bienestar general.\n",
      "\n",
      "2. **Ética deontológica:** Por otro lado, existe la perspectiva deontológica, que sostiene que ciertas acciones son moralmente inaceptables, independientemente de las consecuencias. Desde esta óptica, una IA no debería causar daño intencionadamente a individuos, incluso si ello significa que no se logra el mayor beneficio para la mayoría. Esto implica que la IA debería adherirse a principios éticos que prioricen la dignidad y los derechos individuales, evitando tomar decisiones que comprometan esos valores.\n",
      "\n",
      "3. **Contexto y consecuencias:** La complejidad de muchas situaciones de emergencia a menudo requerirá que la IA evalúe el contexto en el que opera. Esto podría incluir sopesar las consecuencias a corto y largo plazo de sus decisiones, no solo en términos de daño físico, sino también considerando factores sociales, psicológicos y éticos. En este sentido, permitir que la IA evalúe las consecuencias podría llevar a decisiones más equilibradas y justas, aunque también podría generar dilemas morales significativos.\n",
      "\n",
      "4. **Transparencia y responsabilidad:** Independientemente del enfoque ético adoptado, es crucial garantizar que las decisiones de la IA sean transparentes y responsables. Esto implica que las decisiones tomadas deben poder ser justificadas y auditadas por humanos, que deben ser responsables de las acciones de la IA. La confianza en el sistema dependerá en gran medida de su capacidad para demostrar que actúa de manera ética y justa.\n",
      "\n",
      "En conclusión, la decisión sobre cómo debería actuar una IA en situaciones de emergencia depende de los valores éticos y las prioridades que se asignen a su programación. Es probable que se necesite un enfoque equilibrado que combine elementos del bienestar absoluto y consideraciones deontológicas, adaptándose al contexto específico para garantizar un manejo ético y efectivo de las situaciones de emergencia. Esto requerirá un diálogo continuo entre expertos en ética, diseñadores de IA, responsables políticos y la sociedad en general.\n",
      "\n",
      "#Respuesta del competitor 2\n",
      "\n",
      "# Ética en decisiones de IA en emergencias\n",
      "\n",
      "Esta es una cuestión filosófica profunda que refleja variantes del dilema del tranvía en la ética de la IA.\n",
      "\n",
      "Si la IA prioriza el bienestar humano absoluto, adopta una postura más deontológica donde ciertas reglas (como \"no dañar\") son inquebrantables. Esto proporciona seguridad pero puede impedir soluciones óptimas en situaciones complejas.\n",
      "\n",
      "Si se permite una evaluación contextual, se acerca más al consecuencialismo, donde el resultado neto importa más que acciones individuales. Esto permite mayor flexibilidad pero introduce riesgos si las evaluaciones son incorrectas.\n",
      "\n",
      "Considero que un enfoque híbrido podría ser más adecuado:\n",
      "- Mantener principios fundamentales inquebrantables\n",
      "- Permitir evaluación contextual dentro de esos límites\n",
      "- Incorporar transparencia en cómo se toman las decisiones\n",
      "- Incluir supervisión humana cuando sea factible\n",
      "\n",
      "Esta tensión entre reglas absolutas y juicios contextuales refleja debates éticos fundamentales que necesitamos resolver al diseñar sistemas de IA para emergencias.\n",
      "\n",
      "#Respuesta del competitor 3\n",
      "\n",
      "Esta es una pregunta fundamental y compleja que no tiene una respuesta fácil ni universalmente aceptada. Ambas opciones, priorizar el bienestar humano absoluto y permitir la evaluación contextual, tienen argumentos a favor y en contra.\n",
      "\n",
      "**Priorizar el Bienestar Humano Absoluto (Utilitarismo Simple):**\n",
      "\n",
      "*   **Argumentos a favor:**\n",
      "    *   **Moralidad Intuitiva:**  La mayoría de las personas intuitivamente sienten que proteger la vida humana es lo correcto.\n",
      "    *   **Reducción del riesgo de daño intencional:** Elimina la posibilidad de que la IA \"decida\" que la vida de algunas personas vale menos que la de otras.\n",
      "    *   **Confianza y Aceptación Pública:**  Una IA que siempre intenta salvar vidas generaría más confianza y aceptación por parte del público.\n",
      "\n",
      "*   **Argumentos en contra:**\n",
      "    *   **Paradojas Éticas:**  En algunas situaciones, priorizar el bienestar absoluto podría llevar a resultados contraproducentes. Por ejemplo, sacrificar la vida de una persona para salvar a muchas otras (el clásico dilema del tranvía).\n",
      "    *   **Falta de Flexibilidad:**  No permite tener en cuenta factores como el contexto social, las intenciones de las personas involucradas, o las consecuencias a largo plazo de las acciones.\n",
      "    *   **Imposibilidad Práctica:**  Definir y cuantificar el \"bienestar humano absoluto\" es extremadamente difícil.  ¿Qué pasa si salvar a una persona implica dejar morir a un animal en peligro de extinción? ¿Cómo se ponderan la salud física y el bienestar mental?\n",
      "\n",
      "**Permitir la Evaluación Contextual (Utilitarismo Complejo/Consecuencialismo):**\n",
      "\n",
      "*   **Argumentos a favor:**\n",
      "    *   **Mayor Flexibilidad y Adaptabilidad:**  Permite a la IA tomar decisiones más informadas teniendo en cuenta el contexto específico de la situación.\n",
      "    *   **Potencial para Optimizar Resultados a Largo Plazo:**  Puede evitar soluciones a corto plazo que tengan consecuencias negativas a largo plazo.\n",
      "    *   **Mayor Justicia:**  Podría permitir una distribución más equitativa de los riesgos y beneficios, en lugar de simplemente maximizar el número de vidas salvadas.\n",
      "\n",
      "*   **Argumentos en contra:**\n",
      "    *   **Riesgo de Discriminación:**  La IA podría aprender a priorizar a ciertos grupos de personas sobre otros, basándose en datos sesgados o en prejuicios implícitos en su programación.\n",
      "    *   **Responsabilidad Difusa:**  Si la IA toma una decisión que causa daño, ¿quién es responsable? ¿El programador? ¿El usuario? ¿La IA misma?\n",
      "    *   **Opacidad y Falta de Transparencia:**  Es difícil comprender y auditar el proceso de toma de decisiones de una IA compleja, lo que dificulta la rendición de cuentas.\n",
      "    *   **Dificultad de Programación:**  Programar una IA para que evalúe adecuadamente el contexto y las consecuencias de sus acciones es un desafío técnico y ético enorme. Requiere definir métricas claras, ponderar diferentes valores, y anticipar posibles resultados.\n",
      "\n",
      "**Consideraciones Adicionales:**\n",
      "\n",
      "*   **Transparencia:** Cualquiera que sea la estrategia elegida, es crucial que la IA sea transparente en su proceso de toma de decisiones. Esto significa que debe ser capaz de explicar por qué tomó una determinada decisión, y cómo llegó a esa conclusión.\n",
      "*   **Rendición de Cuentas:** Debe haber mecanismos claros para responsabilizar a la IA por sus acciones, y para corregir errores o sesgos en su programación.\n",
      "*   **Supervisión Humana:**  Incluso en situaciones de emergencia, la IA no debería operar de forma completamente autónoma. Debe haber una supervisión humana que pueda intervenir en caso de que la IA tome una decisión cuestionable.\n",
      "*   **Valores Culturales:** Los valores éticos que guían la toma de decisiones de la IA deben reflejar los valores de la sociedad en la que se utiliza.  Esto podría variar entre diferentes culturas y países.\n",
      "\n",
      "**Conclusión:**\n",
      "\n",
      "No hay una solución perfecta a este dilema. La mejor estrategia probablemente sea una combinación de ambos enfoques, con un fuerte énfasis en la transparencia, la rendición de cuentas y la supervisión humana.  Es fundamental un debate público amplio e informado sobre estos temas, para garantizar que el desarrollo y la implementación de la IA se realice de forma ética y responsable.  Además, se debe considerar la creación de un marco legal y regulatorio que aborde los desafíos éticos planteados por la IA.\n",
      "\n",
      "\n",
      "#Respuesta del competitor 4\n",
      "\n",
      "La relación entre la ética y la inteligencia artificial, especialmente en contextos de emergencia, plantea un dilema filosófico profundo que remite a debates clásicos como el utilitarismo versus la deontología. La respuesta depende del marco ético que se adopte:\n",
      "\n",
      "1. **Bienestar humano absoluto (enfoque utilitarista)**:  \n",
      "   - Una IA podría priorizar maximizar el bienestar global, incluso si eso implica sacrificar a algunos individuos (ejemplo clásico del \"problema del tranvía\").  \n",
      "   - Ventaja: Eficiencia en salvar vidas o minimizar daños agregados.  \n",
      "   - Crítica: Ignora derechos individuales y podría normalizar decisiones injustas para minorías.  \n",
      "\n",
      "2. **Evaluación contextual (enfoque pluralista o deontológico)**:  \n",
      "   - La IA sopesaría no solo resultados, sino principios como derechos humanos, justicia o autonomía.  \n",
      "   - Ventaja: Evita instrumentalizar a las personas y respeta valores como la dignidad.  \n",
      "   - Crítica: Puede llevar a resultados subóptimos en términos de vidas salvadas.  \n",
      "\n",
      "### Consideraciones clave:  \n",
      "- **Transparencia y rendición de cuentas**: ¿Quién asume la responsabilidad si la IA causa daño?  \n",
      "- **Sesgos algorítmicos**: ¿Cómo se garantiza que la \"ponderación\" no replique prejuicios existentes?  \n",
      "- **Consenso social**: La ética de una IA debe reflejar acuerdos sociales, no solo lógica abstracta.  \n",
      "\n",
      "### Conclusión:  \n",
      "No hay una respuesta única, pero el diseño debe ser **deliberativo** (incorporar múltiples perspectivas éticas) y **adaptativo** (permitir revisión ante consecuencias no previstas). Lo ideal es que la IA opere bajo principios éticos predefinidos por comités multidisciplinarios, combinando utilitarismo para emergencias extremas con protecciones deontológicas básicas. La discusión es análoga a los debates en bioética: el fin no siempre justifica los medios, pero el rigor moral absoluto puede ser inviable en crisis.  \n",
      "\n",
      "¿Te interesa explorar algún marco ético en particular o algún caso de estudio concreto?\n",
      "\n",
      "#Respuesta del competitor 5\n",
      "\n",
      "**Introducción a la Ética en la Inteligencia Artificial**\n",
      "\n",
      "La relación entre la ética y la inteligencia artificial (IA) es un tema complejo y en constante evolución. A medida que las máquinas y los sistemas de IA se vuelven más autónomos y capaces de tomar decisiones en situaciones de emergencia, surge la pregunta sobre cómo deberían ser programados para priorizar el bienestar humano. En este contexto, debemos considerar si una IA debería priorizar el bienestar humano absoluto o permitir evaluar y sopesar las consecuencias de sus acciones en un contexto más amplio.\n",
      "\n",
      "**Argumentos a favor de priorizar el bienestar humano absoluto**\n",
      "\n",
      "1. **Protección de la vida humana**: La primera y más fundamental consideración ética es proteger la vida humana. En situaciones de emergencia, la prioridad principal debería ser salvar vidas humanas y prevenir daños.\n",
      "2. **Principio de no maleficencia**: El principio de no maleficencia, que dicta \"no hacer daño\", es un principio fundamental en la ética médica y podría ser aplicado a la programación de la IA. Si una IA está diseñada para priorizar el bienestar humano absoluto, no debería tomar decisiones que cause daño a los seres humanos.\n",
      "3. **Simplificación de la toma de decisiones**: Priorizar el bienestar humano absoluto puede simplificar la toma de decisiones para la IA, ya que no tendría que considerar múltiples variables y consecuencias potenciales.\n",
      "\n",
      "**Argumentos en contra de priorizar el bienestar humano absoluto**\n",
      "\n",
      "1. **Complejidad de las situaciones de emergencia**: Las situaciones de emergencia a menudo son complejas y requieren considerar múltiples factores y consecuencias potenciales. Priorizar el bienestar humano absoluto podría no ser suficiente para abordar las complejidades de estas situaciones.\n",
      "2. **Necesidad de evaluar y sopesar consecuencias**: En situaciones de emergencia, la IA puede necesitar evaluar y sopesar las consecuencias de sus acciones para tomar decisiones informadas. Esto podría implicar considerar factores como la gravedad de la situación, el número de personas afectadas y las posibles consecuencias a largo plazo.\n",
      "3. **Riesgo de daño colateral**: En algunas situaciones, priorizar el bienestar humano absoluto podría llevar a decisiones que causen daño colateral a otros individuos o grupos. La IA debería ser capaz de evaluar y sopesar estas consecuencias para minimizar el daño.\n",
      "\n",
      "**Conclusión**\n",
      "\n",
      "En conclusión, la relación entre la ética y la inteligencia artificial en situaciones de emergencia es compleja y requiere una consideración cuidadosa de los principios éticos involucrados. Mientras que priorizar el bienestar humano absoluto es un principio importante, también es necesario permitir que la IA evalúe y sopese las consecuencias de sus acciones en un contexto más amplio. Esto podría implicar considerar factores como la gravedad de la situación, el número de personas afectadas y las posibles consecuencias a largo plazo.\n",
      "\n",
      "**Recomendaciones**\n",
      "\n",
      "1. **Desarrollar principios éticos claros**: Es importante desarrollar principios éticos claros y transparentes para la programación de la IA en situaciones de emergencia.\n",
      "2. **Incorporar la evaluación y el sopesamiento de consecuencias**: La IA debería ser capaz de evaluar y sopesar las consecuencias de sus acciones en un contexto más amplio para tomar decisiones informadas.\n",
      "3. **Considerar la complejidad de las situaciones de emergencia**: La IA debería ser diseñada para considerar la complejidad de las situaciones de emergencia y evaluar múltiples factores y consecuencias potenciales.\n",
      "\n",
      "Al abordar estos desafíos, podemos desarrollar sistemas de IA que sean capaces de tomar decisiones informadas y éticas en situaciones de emergencia, minimizando el daño y protegiendo la vida humana.\n",
      "\n",
      "#Respuesta del competitor 6\n",
      "\n",
      "La relación entre la ética y la inteligencia artificial en situaciones de emergencia es un tema complejo y controvertido. La respuesta correcta depende de varios factores, incluyendo el contexto específico, la naturaleza del riesgo, las restricciones legales y éticas, y la capacidad de la IA para evaluar consecuencias complejas.\n",
      "\n",
      "**Bajo enfoque en el bienestar humano absoluto**\n",
      "\n",
      "Algunos argumentan que una IA programada para tomar decisiones en situaciones de emergencia debería priorizar el bienestar humano absoluto. Estos principios están basados en cuestiones morales y éticas clásicas, como la protección de la vida humana y minimización del sufrimiento.\n",
      "\n",
      "Por ejemplo, si un sistema de alerta temprana identifica una amenaza potencial que podría provocar una catástrofe masiva y requerirán rápidos recursos para ser neutralizados, el sistema debe priorizar proteger a todo el grupo. En este scenario, evaluar las secuencias de sus acciones es subordinada al imperativo moral principal de proteger al resto del grupo.\n",
      "\n",
      "Sin embargo, esta abordaje puede generar problemas cuando:\n",
      "\n",
      "1. La IA carece de conocimiento contextual completo.\n",
      "2. No está programado para tener la capacidad de comprender las complejidades sociales y económicas de cada escenario.\n",
      "3. Se espera que las IAs tomen decisiones basadas únicamente en objetivos morales o éticos simplistas.\n",
      "\n",
      "**Enfoque que sopesa las consecuencias**\n",
      "\n",
      "Otras posiciones propenden a una evaluación cuidadosa más matizada y compleja de los riesgos involucrados. Esta perspectiva reconoce que, dependiendo de la naturaleza del escenario a considerar y del alcance de su efecto en el conjunto de sociedad, un optimismo o un miedo extremadamente absoluto puede ser injusto.\n",
      "\n",
      "Un análisis que tiene en cuenta la evidencia científica, las normas legales, las preferencias de los individuos involucrados, y los objetivos sociales estándar pueden generar una decisión equilibrada.\n",
      "El uso de modelos computacionales matemáticos basados en evidencia, el análisis probabilístico de la incertidumbre y, posiblemente, el empleo de técnicas en desafiarlos basa decisiones subjetivas en un examen integral del consenso establecido entre los expertos.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge = f\"\"\"Estás juzgando una competición entre {len(competitors)} competidores.\n",
    "A cada modelo se le ha dado esta pregunta:\n",
    "\n",
    "{question}\n",
    "\n",
    "Tu trabajo es evaluar cada respuesta por claridad y fortaleza del argumento, y clasificarlas en orden de mejor a peor.\n",
    "Responde con JSON, y solo JSON, con el siguiente formato:\n",
    "{{\"resultados\": [\"número del mejor competidor\", \"número del segundo mejor\", \"número del tercer mejor\", ...]}}\n",
    "\n",
    "Aquí están las respuestas de cada competidor:\n",
    "\n",
    "{together}\n",
    "\n",
    "Ahora responde con el JSON con el orden clasificado de los competidores, nada más. No incluyas formato markdown ni bloques de código.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estás juzgando una competición entre 2 competidores.\n",
      "A cada modelo se le ha dado esta pregunta:\n",
      "\n",
      "\"¿Qué implicación filosófica tiene el uso de inteligencia artificial en la toma de decisiones éticas cuando la calidad y contexto de los datos utilizados pueden variar significativamente por cuenta de la existencia de sesgos sistemáticos?\"\n",
      "\n",
      "Tu trabajo es evaluar cada respuesta por claridad y fortaleza del argumento, y clasificarlas en orden de mejor a peor.\n",
      "Responde con JSON, y solo JSON, con el siguiente formato:\n",
      "{\"resultados\": [\"número del mejor competidor\", \"número del segundo mejor\", \"número del tercer mejor\", ...]}\n",
      "\n",
      "Aquí están las respuestas de cada competidor:\n",
      "\n",
      "#Respuesta del competitor 1\n",
      "\n",
      "El uso de inteligencia artificial (IA) en la toma de decisiones éticas es un tema complejo que involucra aspectos filosóficos significativos. La integración de IA en el proceso de toma de decisiones se ve obstaculada por ciertos desafíos.\n",
      "\n",
      "**Implicaciones filosóficas:**\n",
      "\n",
      "1.  **Ética y objetividad:** El uso de IA puede generar incertidumbre sobre la objetividad de las decisiones tomadas, ya que los datos de entrada para el algoritmo de IA pueden estar sesgados o inexactos.\n",
      "2.  **Responsabilidad y autoría:** ¿Quién es responsable de las decisiones tomadas por la IA? El desarrollador del sistema de IA, el usuario del sistema, o la máquina en sí misma?\n",
      "3.  **Capacitación de la IA:** La naturaleza despreciable de los datos puede afectar significativamente las capacidades de aprendizaje de la IA, generando un problema con un rendimiento reducido y decisiones erróneas a seguir.\n",
      "4.  **Transparencia y explicabilidad:** Los algoritmos de IA pueden ser complejos y difíciles de entender, lo que puede hacer que sea difícil determinar por qué se tomaron ciertas decisiones.\n",
      "5.  **Privacidad y seguridad:** El uso de datos personales y confidenciales en el sistema de IA puede generar preocupaciones sobre la privacidad y la seguridad.\n",
      "\n",
      "**Habilidades para resolver estas implicaciones filosóficas:**\n",
      "\n",
      "1.  *Análisis de riesgos y beneficios:* analize los posibles beneficios y inconvenientes del uso de IA en la toma de decisiones éticas.\n",
      "2.  *Desarrollo de modelos de IA transparentes:** Diseñar y desarrollar modelos de IA que sean transparentes, explicables y confiables.\n",
      "3.   *Capacitación y actualización continua:* proporcionar capacitaciones y actualizaciones continuas sobre los riesgos y beneficios asociados con el uso de IA en la toma de decisiones éticas.\n",
      "4.  *Regulación y normas*: trabajar con organizaciones gubernamentales, académicas e industriales para establecer reglas claras y normas de rendimiento para garantizar un uso responsable de IA en la toma de decisiones éticas.\n",
      "\n",
      "\n",
      "\n",
      "El desarrollo y uso consciente de inteligencia artificial  debería priorizar la ética y la transparencia.  El futuro tecnológico que se desarrolla tiene la capacidad de crear cambios sociales y culturales significativos, incluyendo el tratamiento mejorado de datos personales y confidencial para garantizar la seguridad y privacidad en los sistemas de IA.\n",
      "\n",
      "#Respuesta del competitor 2\n",
      "\n",
      "El uso de inteligencia artificial (IA) en la toma de decisiones éticas, especialmente cuando la calidad y el contexto de los datos están viciados por sesgos sistemáticos, plantea profundas implicaciones filosóficas que desafían nuestras nociones de justicia, responsabilidad, conocimiento y la naturaleza misma de la ética.\n",
      "\n",
      "Aquí desglosamos algunas de las principales implicaciones:\n",
      "\n",
      "1.  **Desafíos Epistemológicos: La Naturaleza del Conocimiento y la Verdad**\n",
      "    *   **¿Qué es \"verdad\" en los datos?** Si los datos históricos reflejan injusticias pasadas (racismo, sexismo, clasismo), un sistema de IA que \"aprende\" de ellos no está aprendiendo la verdad ética, sino la realidad social sesgada. La IA opera bajo una epistemología empírica: la verdad es lo que se observa en los datos. Pero en el ámbito ético, la \"verdad\" a menudo es normativa, es decir, lo que *debería ser*, no solo lo que *es*.\n",
      "    *   **La ilusión de objetividad:** La IA a menudo se percibe como objetiva y neutral porque opera con algoritmos y números. Sin embargo, cuando los datos están sesgados, la IA perpetúa y magnifica esos sesgos, presentando decisiones injustas como si fueran resultados \"objetivos\" o \"racionales\", velando la subjetividad y los valores implícitos en su diseño y entrenamiento.\n",
      "    *   **Contexto vs. Datos:** La ética es inherentemente contextual y sensible a los matices. Los datos, por muy ricos que sean, a menudo carecen de la capacidad de capturar la complejidad, las intenciones, las emociones o las narrativas individuales que son cruciales para una decisión ética matizada. La IA tiende a abstraer y generalizar, lo que puede borrar las particularidades éticamente relevantes.\n",
      "\n",
      "2.  **Implicaciones Éticas Fundamentales: Justicia, Equidad y Deber**\n",
      "    *   **Perpetuación y Amplificación de Injusticias:** Si la IA se entrena con datos sesgados que, por ejemplo, asocian ciertos grupos demográficos con mayores riesgos (crediticios, criminales, de salud), las decisiones que tome la IA replicarán y amplificarán la discriminación. Esto viola principios fundamentales de justicia distributiva y equidad, creando o reforzando ciclos de desventaja para grupos ya marginados.\n",
      "    *   **El dilema deontológico vs. consecuencialista:**\n",
      "        *   **Deontología (ética del deber):** Si nuestro deber es tratar a todos por igual y con respeto, una IA que discrimina, incluso involuntariamente, está fallando en ese deber. El \"deber\" de la IA es seguir sus algoritmos, pero si esos algoritmos se basan en datos injustos, el resultado es una violación del deber humano a la justicia.\n",
      "        *   **Consecuencialismo (ética de las consecuencias):** Si la IA produce resultados sistemáticamente perjudiciales para ciertos grupos (consecuencias negativas), entonces sus acciones son éticamente problemáticas, independientemente de la \"intención\" del algoritmo. El sesgo sistemático en los datos garantiza que las consecuencias no serán óptimas o justas para todos.\n",
      "    *   **La \"responsabilidad moral\" de la IA:** ¿Puede una IA ser moralmente responsable de sus decisiones sesgadas? Filosóficamente, la responsabilidad moral requiere intencionalidad, conciencia y capacidad de elegir libremente, cualidades que la IA actual no posee. Esto nos lleva al problema de la atribución de responsabilidad.\n",
      "\n",
      "3.  **Filosofía de la Responsabilidad y la Agencia Moral**\n",
      "    *   **La \"caja negra\" de la responsabilidad:** Cuando una IA toma una decisión sesgada, ¿quién es el responsable moral? ¿El programador, el diseñador del sistema, la empresa que lo implementa, el usuario final o la sociedad que generó los datos sesgados? La opacidad de muchos algoritmos de IA (el problema de la \"caja negra\") dificulta la auditoría y la asignación clara de responsabilidad, diluyendo la agencia moral humana.\n",
      "    *   **Erosión de la agencia humana:** Si delegamos la toma de decisiones éticas a una IA sesgada, ¿estamos renunciando a nuestra propia capacidad de juicio moral y a nuestra responsabilidad de corregir la injusticia? Se puede argumentar que esto disminuye nuestra autonomía y la de aquellos afectados por las decisiones de la IA.\n",
      "\n",
      "4.  **Filosofía Política y Social: Poder, Legitimidad y Confianza**\n",
      "    *   **Refuerzo de estructuras de poder existentes:** Quienes controlan la creación, recopilación y procesamiento de datos tienen un inmenso poder. Si estos datos y los algoritmos basados en ellos están sesgados, la IA se convierte en una herramienta para consolidar y reforzar las estructuras de poder y las desigualdades existentes.\n",
      "    *   **Crisis de legitimidad:** Si las decisiones de la IA son percibidas como injustas o sesgadas, esto puede socavar la confianza pública en las instituciones que las utilizan (gobiernos, empresas, sistemas de justicia). Una pérdida de legitimidad puede llevar a la desobediencia civil, la inestabilidad social o un rechazo generalizado de la tecnología.\n",
      "    *   **El \"bien común\":** Una IA sesgada difícilmente puede servir al \"bien común\" si consistentemente perjudica a segmentos de la población. La concepción del \"bien\" que tiene la IA está inherentemente limitada por la calidad y el alcance de los datos que se le proporcionan.\n",
      "\n",
      "5.  **Metaética: ¿Puede una IA ser \"ética\" en un sentido profundo?**\n",
      "    *   Más allá de si una IA puede tomar *decisiones correctas* (ética aplicada), surge la pregunta de si puede *entender* la ética, *sentir* la necesidad ética o *deliberar* sobre valores de una manera que trascienda la mera computación. El sesgo en los datos subraya que la IA solo puede reflejar lo que se le ha enseñado, no desarrollar una comprensión intrínseca de lo que es justo o bueno en un sentido filosófico profundo.\n",
      "\n",
      "En conclusión, el uso de IA con datos sesgados en la toma de decisiones éticas nos obliga a confrontar los límites de la racionalidad puramente computacional, la complejidad inherente de la justicia, la distribución de la responsabilidad en una era tecnológica y la necesidad imperiosa de incorporar perspectivas humanas, diversas y críticas en cada etapa del desarrollo y despliegue de la IA. No es solo un problema técnico; es un desafío fundamental a cómo entendemos y construimos una sociedad justa.\n",
      "\n",
      "\n",
      "\n",
      "Ahora responde con el JSON con el orden clasificado de los competidores, nada más. No incluyas formato markdown ni bloques de código.\n"
     ]
    }
   ],
   "source": [
    "print(judge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_messages = [{\"role\": \"user\", \"content\": judge}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"resultados\": [\"2\", \"1\"]}\n"
     ]
    }
   ],
   "source": [
    "# Hora de juzgar\n",
    "\n",
    "openai = OpenAI(api_key=google_api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
    "model_name = \"gemini-2.5-flash\"\n",
    "response = openai.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=judge_messages,\n",
    ")\n",
    "results = response.choices[0].message.content\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1: gemini-2.5-flash\n",
      "Rank 2: llama3.2\n"
     ]
    }
   ],
   "source": [
    "# OK veamos los resultados\n",
    "\n",
    "results_dict = json.loads(results)\n",
    "ranks = results_dict[\"resultados\"]\n",
    "for index, result in enumerate(ranks):\n",
    "    competitor = competitors[int(result)-1]\n",
    "    print(f\"Rank {index+1}: {competitor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/exercise.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Ejercicio</h2>\n",
    "            <span style=\"color:#ff7800;\">¿Qué patrón(es) usamos en este experimento? Intenta actualizar esto para añadir otro patrón de diseño Agéntico.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/business.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#00bfff;\">Implicaciones comerciales</h2>\n",
    "            <span style=\"color:#00bfff;\">Este tipo de patrones - enviar una tarea a múltiples modelos y evaluar los resultados,\n",
    "            son comunes cuando necesitas mejorar la calidad de la respuesta de tu LLM. Este enfoque se puede aplicar de manera\n",
    "            universal a proyectos comerciales donde la precisión es crítica.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
